{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Predicting Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to train a single layer neural network classifier. The model will have 9 inputs and 2 outputs, and will be fed features related to cell attributes. The model will then output a probability distribution over the events of the cell being benign or the cell being malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collecting our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset from the UCI Machine Learning Repository. It can be found <a href=\"https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28original%29\">here.</a>\n",
    "\n",
    "First, our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the csv data, we will to parse it and convert it into a numpy matrix. We can delete the first feature of the dataset, which is the sample code. This feature is meaningless to us, we will also convert our code specifying malignancy from 2 and 4 to 0 and 1. This will make it easier to use the cross entropy function to compare our probability distributions and train the model. Also, we will delete any rows with missing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First feature vector of dataset: [5 1 1 1 2 1 3 1 1]\n",
      "First class label of dataset: [0 1] (not cancerous)\n",
      "Sixth feature vector of dataset: [ 8 10 10  8  7 10  9  7  1]\n",
      "First class label of dataset: [1 0] (cancerous)\n",
      "total length of dataset: 121\n"
     ]
    }
   ],
   "source": [
    "def training_data():\n",
    "    with open('breast_cancer_data.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        bc_training_data = []\n",
    "        bc_training_labels = []\n",
    "        for row in csv_reader:\n",
    "            try:\n",
    "                integer_mapped = map(int, row[:-1])\n",
    "                bc_training_data.append(integer_mapped[:-1])\n",
    "                if(integer_mapped[-1] == 0):\n",
    "                    bc_training_labels.append(np.array([0,1]))\n",
    "                else:\n",
    "                    bc_training_labels.append(np.array([1,0]))\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return np.array(bc_training_data), np.array(bc_training_labels)\n",
    "\n",
    "data_x, data_y = training_data()\n",
    "print 'First feature vector of dataset: {}'.format(data_x[0])\n",
    "print 'First class label of dataset: {} (not cancerous)'.format(data_y[0])\n",
    "\n",
    "print 'Sixth feature vector of dataset: {}'.format(data_x[5])\n",
    "print 'First class label of dataset: {} (cancerous)'.format(data_y[5])\n",
    "\n",
    "print 'total length of dataset: {}'.format(len(data_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then separate the data into a training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training set: 90\n",
      "length of testing set: 31\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x = data_x[:90], data_x[90:]\n",
    "train_y, test_y = data_y[:90], data_y[90:]\n",
    "print 'length of training set: {}'.format(len(train_x))\n",
    "print 'length of testing set: {}'.format(len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up our tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will instantiate our two placeholders needed; the placeholder for the matrix of feature vectors and the placeholder for the matrix of class labels. We set the vertical size of the matrices to None because we want to be able to feed an arbitrary amount of feature vectors into the classifier for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype = tf.float32, shape = [None,9])\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set up  our tensorflow variables to be initialized. We will only need one weight matrix since this is a linear classifier. Also, a bias vector will be declared. A bias is important to allow our separating plane move about as well. \n",
    "\n",
    "We then define the computation to get our matrix of outputs corresponding to each input. We do this by multiplying the input and weight matrix, adding the bias, then applying the softmax function. We apply the softmax function to each row in the output matrix to normalize the values and get probability distributions.\n",
    "\n",
    "We will then define our cross function as the mean of the cross entropy values between the predicted labels and the correct labels. Also, a gradient descent optimizer will be declared and used for training.\n",
    "\n",
    "To benchmark our model, we will also compute a vector of True/False values corresponding to correct class predictions, then convert it to a vector of 1's and 0's the compute our score as a percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-243eeb4e5ad7>:6: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "weights = tf.Variable(tf.random_normal([9, 2]))\n",
    "bias = tf.Variable(tf.zeros([1,2]))\n",
    "\n",
    "y_predictions = tf.nn.softmax(tf.matmul(x, weights) + bias)\n",
    "\n",
    "y_predicted_classes = tf.argmax(y_predictions, dimension=1)\n",
    "y_true_class = tf.argmax(y, dimension=1)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_predictions, labels=y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_predicted_classes, y_true_class), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will defined a training function taking an amount of epochs as input. We will train over the dataset in full batches. A test function to check performance will also be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    feed_dict={x:train_x, y:train_y}\n",
    "    for i in xrange(epochs):\n",
    "        session.run(optimizer, feed_dict = feed_dict)\n",
    "        \n",
    "def test():\n",
    "    acc = session.run(accuracy, feed_dict={x:test_x, y:test_y})\n",
    "    print \"Accuracy on test-set: {0:.1%}\".format(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model first without any training, train it for 9000 epochs, then test it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 48.4%\n",
      "Accuracy on test-set: 93.5%\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "test()\n",
    "train(9000)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying a couple times, we are able to achieve an accuracy at 93.5%. Though our model is fairly accurate, there are definitely avenues for improvement we can take later, perhaps in another notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
